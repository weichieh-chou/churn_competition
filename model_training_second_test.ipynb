{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbaa6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ada1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_DIR = Path(\"data/processing_checkpoint\")\n",
    "checkpoint_file_path = PROCESSED_DATA_DIR / \"03_10_day_window_sliced.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c1b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_parquet(checkpoint_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a897c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_numerical_features = [\n",
    "    \"count_about\", \"count_total_sessions\", \"count_downgrade\", \"count_error\",\n",
    "    \"thumbs_ratio\", \"ads_per_session\", \"frequency\", \"avg_songs_session\", \n",
    "    \"errors_per_session\", \"user_lifecycle_h\", \"count_roll_advert\", \"count_upgrade\",\n",
    "    \"session_length_variance\", \"active_days_ratio\", \"hours_since_last_session\",\n",
    "    \"unique_songs_ratio\", \"hours_since_downgrade\", \"is_new_user\"\n",
    "]\n",
    "\n",
    "final_categorical_features = [\"last_level\"]\n",
    "\n",
    "all_features = final_numerical_features + final_categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58bf364",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final[all_features]\n",
    "y = df_final[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46a075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf3fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_numerical_subset = X[final_numerical_features]\n",
    "\n",
    "# correlation_matrix = df_numerical_subset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcb1bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d6194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(18, 15))\n",
    "\n",
    "# # Create the heatmap\n",
    "# sns.heatmap(\n",
    "#     correlation_matrix,\n",
    "#     annot=True,         # Show the correlation numbers on the map\n",
    "#     cmap='coolwarm',    # Color map: cool=negative, warm=positive\n",
    "#     fmt=\".2f\",          # Format the numbers to two decimal places\n",
    "#     linewidths=0.5,     # Add lines between cells for clarity\n",
    "#     linecolor='black',\n",
    "#     cbar_kws={'shrink': 0.8} # Shrink color bar slightly\n",
    "# )\n",
    "\n",
    "# plt.title('Feature Correlation Matrix (Final Numerical Set)', fontsize=16)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc2dbdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate\n",
    "from sklearn.metrics import f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dafc7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", drop='first'), final_categorical_features),\n",
    "        (\"num\", StandardScaler(), final_numerical_features)\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c968e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5c03cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_pipeline = ImbPipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    (\"classifier\", lr_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "240838d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "scoring = {'F1': make_scorer(f1_score), 'AUC': make_scorer(roc_auc_score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0d834a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_scores = cross_validate(\n",
    "    gbdt_pipeline, \n",
    "    X, \n",
    "    y, \n",
    "    scoring=scoring, \n",
    "    cv=tscv, \n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b884342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT Mean F1 Score (Validation): 0.1709\n",
      "GBDT Mean AUC Score (Validation): 0.6617\n"
     ]
    }
   ],
   "source": [
    "print(f\"GBDT Mean F1 Score (Validation): {gbdt_scores['test_F1'].mean():.4f}\")\n",
    "print(f\"GBDT Mean AUC Score (Validation): {gbdt_scores['test_AUC'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fb9956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=== DIAGNOSTIC REPORT ===\")\n",
    "# print(f\"Dataset shape: {df_final.shape}\")\n",
    "# print(f\"Feature matrix shape: {X.shape}\")\n",
    "# print(f\"Churn rate: {y.mean():.2%}\")\n",
    "# print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "# print(f\"Constant features: {(X.nunique() == 1).sum()}\")\n",
    "# print(f\"\\nSnapshot days: {sorted(df_final['snapshot_day'].unique())}\")\n",
    "# print(f\"Users per snapshot: {df_final.groupby('snapshot_day')['userId'].nunique().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad9fe81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "#     print(f\"\\nFold {fold + 1}:\")\n",
    "#     print(f\"  Train: {len(train_idx)} samples, Churn rate: {y.iloc[train_idx].mean():.2%}\")\n",
    "#     print(f\"  Val: {len(val_idx)} samples, Churn rate: {y.iloc[val_idx].mean():.2%}\")\n",
    "    \n",
    "#     # Check if validation set has ANY churners\n",
    "#     if y.iloc[val_idx].sum() == 0:\n",
    "#         print(\"  ⚠️ WARNING: No churners in validation set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aba432e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Check TimeSeriesSplit behavior\n",
    "# print(\"=== TIME SERIES SPLIT ANALYSIS ===\")\n",
    "# tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "#     print(f\"\\nFold {fold + 1}:\")\n",
    "#     print(f\"  Train size: {len(train_idx)}, Churn rate: {y.iloc[train_idx].mean():.2%}\")\n",
    "#     print(f\"  Val size: {len(val_idx)}, Churn rate: {y.iloc[val_idx].mean():.2%}\")\n",
    "#     print(f\"  Val churners: {y.iloc[val_idx].sum()}\")\n",
    "    \n",
    "#     # Check snapshot day distribution in train/val\n",
    "#     train_days = df_final.iloc[train_idx]['snapshot_day'].value_counts().sort_index()\n",
    "#     val_days = df_final.iloc[val_idx]['snapshot_day'].value_counts().sort_index()\n",
    "#     print(f\"  Train snapshot days: {train_days.to_dict()}\")\n",
    "#     print(f\"  Val snapshot days: {val_days.to_dict()}\")\n",
    "\n",
    "# # 2. Check individual feature predictive power\n",
    "# print(\"\\n=== FEATURE PREDICTIVE POWER ===\")\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# feature_aucs = {}\n",
    "# for col in final_numerical_features:\n",
    "#     if X[col].nunique() > 1:\n",
    "#         try:\n",
    "#             # Handle NaN and inf\n",
    "#             valid_mask = np.isfinite(X[col])\n",
    "#             if valid_mask.sum() > 0:\n",
    "#                 auc = roc_auc_score(y[valid_mask], X[col][valid_mask])\n",
    "#                 # Flip if < 0.5 (negative correlation is still predictive)\n",
    "#                 auc = max(auc, 1 - auc)\n",
    "#                 feature_aucs[col] = auc\n",
    "#         except Exception as e:\n",
    "#             print(f\"{col}: Error - {e}\")\n",
    "\n",
    "# # Sort by predictive power\n",
    "# feature_aucs_sorted = sorted(feature_aucs.items(), key=lambda x: x[1], reverse=True)\n",
    "# print(\"\\nTop 10 most predictive features:\")\n",
    "# for feat, auc in feature_aucs_sorted[:10]:\n",
    "#     print(f\"  {feat}: {auc:.4f}\")\n",
    "\n",
    "# print(\"\\nBottom 5 least predictive features:\")\n",
    "# for feat, auc in feature_aucs_sorted[-5:]:\n",
    "#     print(f\"  {feat}: {auc:.4f}\")\n",
    "\n",
    "# # 3. Check if there's overlap in users between train/val\n",
    "# print(\"\\n=== USER OVERLAP CHECK ===\")\n",
    "# for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "#     train_users = set(df_final.iloc[train_idx]['userId'])\n",
    "#     val_users = set(df_final.iloc[val_idx]['userId'])\n",
    "#     overlap = len(train_users & val_users)\n",
    "#     print(f\"Fold {fold + 1}: {overlap} users appear in both train and val (out of {len(val_users)} val users)\")\n",
    "#     if overlap > 0:\n",
    "#         print(f\"  ⚠️ WARNING: {overlap/len(val_users):.1%} of validation users also in training!\")\n",
    "\n",
    "# # 4. Check label distribution across snapshot days\n",
    "# print(\"\\n=== LABEL DISTRIBUTION BY SNAPSHOT DAY ===\")\n",
    "# print(df_final.groupby('snapshot_day')['label'].agg(['count', 'sum', lambda x: f\"{x.mean():.2%}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac923b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
